{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves.urllib import request\n",
    "\n",
    "import chainer\n",
    "from chainer import computational_graph\n",
    "from chainer import cuda\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import serializers\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parent = 'http://yann.lecun.com/exdb/mnist'\n",
    "train_images = 'train-images-idx3-ubyte.gz'\n",
    "train_labels = 'train-labels-idx1-ubyte.gz'\n",
    "test_images = 't10k-images-idx3-ubyte.gz'\n",
    "test_labels = 't10k-labels-idx1-ubyte.gz'\n",
    "num_train = 60000\n",
    "num_test = 10000\n",
    "dim = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(images, labels, num):\n",
    "    data = np.zeros(num * dim, dtype=np.uint8).reshape((num, dim))\n",
    "    target = np.zeros(num, dtype=np.uint8).reshape((num, ))\n",
    "\n",
    "    with gzip.open(images, 'rb') as f_images,\\\n",
    "            gzip.open(labels, 'rb') as f_labels:\n",
    "        f_images.read(16)\n",
    "        f_labels.read(8)\n",
    "        for i in six.moves.range(num):\n",
    "            target[i] = ord(f_labels.read(1))\n",
    "            for j in six.moves.range(dim):\n",
    "                data[i, j] = ord(f_images.read(1))\n",
    "\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_mnist_data():\n",
    "    print('Downloading {:s}...'.format(train_images))\n",
    "    request.urlretrieve('{:s}/{:s}'.format(parent, train_images), train_images)\n",
    "    print('Done')\n",
    "    print('Downloading {:s}...'.format(train_labels))\n",
    "    request.urlretrieve('{:s}/{:s}'.format(parent, train_labels), train_labels)\n",
    "    print('Done')\n",
    "    print('Downloading {:s}...'.format(test_images))\n",
    "    request.urlretrieve('{:s}/{:s}'.format(parent, test_images), test_images)\n",
    "    print('Done')\n",
    "    print('Downloading {:s}...'.format(test_labels))\n",
    "    request.urlretrieve('{:s}/{:s}'.format(parent, test_labels), test_labels)\n",
    "    print('Done')\n",
    "\n",
    "    print('Converting training data...')\n",
    "    data_train, target_train = load_mnist(train_images, train_labels,\n",
    "                                          num_train)\n",
    "    print('Done')\n",
    "    print('Converting test data...')\n",
    "    data_test, target_test = load_mnist(test_images, test_labels, num_test)\n",
    "    mnist = {'data': np.append(data_train, data_test, axis=0),\n",
    "             'target': np.append(target_train, target_test, axis=0)}\n",
    "\n",
    "    print('Done')\n",
    "    print('Save output...')\n",
    "    with open('mnist.pkl', 'wb') as output:\n",
    "        six.moves.cPickle.dump(mnist, output, -1)\n",
    "    print('Done')\n",
    "    print('Convert completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    if not os.path.exists('mnist.pkl'):\n",
    "        download_mnist_data()\n",
    "    with open('mnist.pkl', 'rb') as mnist_pickle:\n",
    "        mnist = six.moves.cPickle.load(mnist_pickle)\n",
    "    return mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MnistMLP(chainer.Chain):\n",
    "\n",
    "    \"\"\"An example of multi-layer perceptron for MNIST dataset.\n",
    "\n",
    "    This is a very simple implementation of an MLP. You can modify this code to\n",
    "    build your own neural net.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_units, n_out):\n",
    "        super(MnistMLP, self).__init__(\n",
    "            l1=L.Linear(n_in, n_units),\n",
    "            l2=L.Linear(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_out),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MnistMLPParallel(chainer.Chain):\n",
    "\n",
    "    \"\"\"An example of model-parallel MLP.\n",
    "\n",
    "    This chain combines four small MLPs on two different devices.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_units, n_out):\n",
    "        super(MnistMLPParallel, self).__init__(\n",
    "            first0=MnistMLP(n_in, n_units // 2, n_units).to_gpu(0),\n",
    "            first1=MnistMLP(n_in, n_units // 2, n_units).to_gpu(1),\n",
    "            second0=MnistMLP(n_units, n_units // 2, n_out).to_gpu(0),\n",
    "            second1=MnistMLP(n_units, n_units // 2, n_out).to_gpu(1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # assume x is on GPU 0\n",
    "        x1 = F.copy(x, 1)\n",
    "\n",
    "        z0 = self.first0(x)\n",
    "        z1 = self.first1(x1)\n",
    "\n",
    "        # sync\n",
    "        h0 = z0 + F.copy(z1, 0)\n",
    "        h1 = z1 + F.copy(z0, 1)\n",
    "\n",
    "        y0 = self.second0(F.relu(h0))\n",
    "        y1 = self.second1(F.relu(h1))\n",
    "\n",
    "        # sync\n",
    "        y = y0 + F.copy(y1, 0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch MNIST dataset\n",
      "load MNIST dataset\n"
     ]
    }
   ],
   "source": [
    "# 確率的勾配降下法で学習させる際の１回分のバッチサイズ\n",
    "batchsize = 100\n",
    "# 学習の繰り返し回数\n",
    "n_epoch   = 20\n",
    "# 中間層の数\n",
    "n_units   = 1000\n",
    "\n",
    "# MNISTの手書き数字データのダウンロード\n",
    "# #HOME/scikit_learn_data/mldata/mnist-original.mat にキャッシュされる\n",
    "print('fetch MNIST dataset')\n",
    "print('load MNIST dataset')\n",
    "mnist = load_mnist_data()\n",
    "# mnist.data : 70,000件の784次元ベクトルデータ\n",
    "mnist['data'] = mnist['data'].astype(np.float32)\n",
    "mnist['data'] /= 255 # 0-1のデータに変換\n",
    "# mnist.target : 正解データ（教師データ）\n",
    "mnist['target'] = mnist['target'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-c8376d1536a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 学習用データを N個、検証用データを残りの個数と設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mN_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# 学習用データを N個、検証用データを残りの個数と設定\n",
    "N = 60000\n",
    "x_train, x_test = np.split(mnist.data,   [N])\n",
    "y_train, y_test = np.split(mnist.target, [N])\n",
    "N_test = y_test.size\n",
    "\n",
    "# Prepare multi-layer perceptron model\n",
    "# 多層パーセプトロンモデルの設定\n",
    "# 入力 784次元、出力 10次元\n",
    "model = FunctionSet(l1=F.Linear(784, n_units),\n",
    "                    l2=F.Linear(n_units, n_units),\n",
    "                    l3=F.Linear(n_units, 10))\n",
    "\n",
    "# Neural net architecture\n",
    "# ニューラルネットの構造\n",
    "def forward(x_data, y_data, train=True):\n",
    "    x, t = Variable(x_data), Variable(y_data)\n",
    "    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n",
    "    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n",
    "    y  = model.l3(h2)\n",
    "    # 多クラス分類なので誤差関数としてソフトマックス関数の\n",
    "    # 交差エントロピー関数を用いて、誤差を導出\n",
    "    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAAC1CAYAAABMMl33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABLpJREFUeJzt3b8va3EYx3G9uUsNJFgsTKYTg4Sw2PoHCInEYJBYDISQ\niGBmNBiNSBiFwY/ZIMKiMZAIm8VCQiTSO/XmNs9z20PPp+e0fb+mm+fq/R4377T5OvVtKpfL5RoA\noV9xXwBqX9HIstlsRS6CdZK7ThRrEBnryNfg5RJyRAa5FLtLqP0u9QWpVKoS14Ea8L/nK14uIUdk\nkCMyyBEZ5IgMckQGOSKDHJFBjsggR2SQIzLIFdy7zGazBe8fGh0drfgFobrt7+///XMQBA1BEJR+\nFwY3yBEWN8gRGyKDHJFBjsggR2SQIzLIERnkiAxyRAY5IoMckUGOyCBHZJAjMsgRGeSIDHJEBjki\ngxyRQY7IIEdkkCMyyBEZ5EoeTIxk6uzsNLN0Om1mY2Nj7uOnpqZCrXN0dGRmExMToR6bxzMZ5IgM\nckQGOSKDHJFBjqOjEiaTyZjZ8PCwmXm7xubmZjMr96OzBgYGvvX13tFRBZHlh8BPeU9MvFxCjsgg\nR2SQIzLIce+yAra2ttx5d3e3mfX19f14ndfXVzPb2dlxv/by8tLMdnd3zezj4+PH15PHMxnkiAxy\nRAY5IoMckUGOyCDHx96UobW11czW1tbMbHJy0n38y8uLmT08PJjZ+vq6md3c3JjZ+/u7mT09Pblr\nK/CxN4gNkUGOyCBHZJAjMsixuyzDxsaGmU1PT5vZ5uam+/jl5WUze3t7K//CYsLuErEhMsgRGeSI\nDHJEBrm63102Njaa2eLiopmNj4+b2ezsrJl5/1/Hx8fu2lG8tTlJ2F0iNkQGOSKDHJFBjsggV/dH\nR62srJiZt7v890ikvJOTEzOrtR3jd3F0FOQ4OgqxIDLIERnkiAxydX/v0vv2vdnQ0JCZHRwcSK6p\nWnHvErEhMsgRGeSIDHJEBjkig1zdn359cXFhZr29vWbm/YKud1TT6elpNBdWQ3gmgxyRQY7IIEdk\nkCMyyFX9DfL+/n4zu76+NrPPz0/38S0tLWY2MzNjZqurq2bmHfPkfdLt7e2tu3at4QY5YkNkkCMy\nyBEZ5IgMcondXba3t5vZ4eGhmXV0dJjZ3NycmW1vb4deu62tzcyen59DPXZwcNDMzs/PQ69dzdhd\nIjZEBjkigxyRQY7IIJfYo6Ourq7MrKmpycy8Y56+s5P0eAcOe87OzszM+7DTesLRUZDj6CjEgsgg\nR2SQIzLIJfbe5dLSkpl5hwin0+my1rm7uzOzrq4uM3t8fDSzkZERM/N2xfWCe5eIDZFBjsggR2SQ\nIzLIERnkEvsjDM/CwoKZ9fT0mFkmkwn9b3rfn3ec1Pz8vJnd39+b2dfXV+i1aw0/wkBsiAxyRAY5\nIoMckUGuqnaXSDZ2l4gNkUGOyCBHZJAjMsgRGeSIDHJEBjkigxyRQY7IIJfYo6NQnbyjo7hBjshw\ngxyxITLIERnkiAxyRAY5IoMckUGOyCBHZJAjMsgRGeSIDHJEBjkigxyRQY7IIEdkkCMyyBEZ5IgM\nckQGOSKDHJFBjsggR2SQIzLIERnkiAxyRAa5kkdHlTj0ByjgHR3VkCtib2+v2F9HhnWSu04Ua/By\nCTkig1zRyIIgqMhFsE5y14lijZLHeQLl4uUScn8A6jy0OprVMzwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125aed860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAAC1CAYAAABMMl33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125aed8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAAC1CAYAAABMMl33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABAVJREFUeJzt3D1Ka1sch2G9WtkenYOQUksJOAQtMgHFOWgt1naCM1Ak\nc9BgY6FV1DlIpqC3uJwLYS3NRvPLh3meSv7n4N7Iywo7K1nLHx8fH0sQ9M+0b4Df78vI+v3+RG7C\ndWb3OuO4hshcJ34NL5fEiYy4ZU+XpK2O+g/Ly8uTuA9+gc/WKy+XxImMOJERJzLiREacyIgTGXEi\nI05kxImMOJERN7R32e/3hz4/1Ol0Jn5DzLfr6+v/f261WkutVmv0pzBskNOUDXKmRmTEiYw4kRE3\n8pOxi6jdbhezm5ubYnZ2dlbMzs/PI/c0z6xkxImMOJERJzLiREacyIizd1lxdHRUzC4uLopZ7U+3\nurq47wrZu2RqREacyIgTGXEiI25xH4W+0Ov1itlgMChmf/78KWYbGxvF7O3tbTw3NqesZMSJjDiR\nEScy4kRGnKfLipeXl2LW7XaL2eHhYTHb29srZpeXl+O5sTllJSNOZMSJjDiREScy4hwd1VDtE8JN\nZ4ukdnTUUGR/h/BdtYXJyyVxIiNOZMSJjDh7lw3VvlPYdLborGTEiYw4kREnMuJERpzIiHN0VENb\nW1vF7OHhoZg9PT0Vs+3t7cg9zRpHRzE1IiNOZMSJjDiREWeD/AdskDdjJSNOZMSJjDiREScy4jxd\n/oAv9zZjJSNOZMSJjDiREScy4hwd1VDtwOHaPuXz8/MkbmdmOTqKOEdHMRUiI05kxImMOHuXDa2v\nrxez2j7l/f39JG5nrljJiBMZcSIjTmTEiYw4kRHnLYwfsEHejJWMOJERJzLiREacyIhzMHFDtT/T\n+/t7MVtZWZnE7cwkBxMzNSIjTmTEiYw4kRFn77Jif3+/mNWeJB1C3IyVjDiREScy4kRGnMiIG9q7\nrB0dZe/yP7e3t8VsZ2enmC363mXt6Cgb5A2JbDQb5EyNyIgTGXEiI87e5Q/Yu2zGSkacyIgTGXEi\nI05kxImMOG9hNPT6+lrManuX7Xa7mN3d3UXuaV5YyYgTGXEiI05kxImMOE+XDXW73WJ2cHBQzDY3\nN4uZp0sIExlxIiNOZMSJjDhPlw0NBoNiVvtO6unpaTFbW1ur/s7Hx8di9hufRK1kxImMOJERJzLi\nREaco6Maqj0hHh8fF7OTk5Ni9tmXgHd3d4tZr9f7xt3Nhs+Ojhp6C+PvEL6r0+kUMy+XxImMOJER\nJzLinBnL2DgzlqkRGXEiI05kxImMOJERJzLiREacyIgTGXEiI05kxImMOJERJzLiREacyIgTGXEi\nI05kxImMOJERJzLiREbc0Kk+taOjRnz3F4bUjo5a+vjC1dXVV/88Nq4zu9cZxzW8XBInMuK+jGxS\npy66zuxeZxzXGHmqD/yUl0vi/gUamHiblCMHwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1405adeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 手書き数字データを描画する関数\n",
    "def draw_digit(data):\n",
    "    size = 28\n",
    "    plt.figure(figsize=(2.5, 3))\n",
    "\n",
    "    X, Y = np.meshgrid(range(size),range(size))\n",
    "    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n",
    "    Z = Z[::-1,:]             # flip vertical\n",
    "    plt.xlim(0,27)\n",
    "    plt.ylim(0,27)\n",
    "    plt.pcolor(X, Y, Z)\n",
    "    plt.gray()\n",
    "    plt.tick_params(labelbottom=\"off\")\n",
    "    plt.tick_params(labelleft=\"off\")\n",
    "    plt.show()\n",
    "\n",
    "draw_digit(mnist['data'][11])\n",
    "draw_digit(mnist['data'][12345])\n",
    "draw_digit(mnist['data'][33456])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 学習用データを N個、検証用データを残りの個数と設定\n",
    "N = 60000\n",
    "gpu = -1\n",
    "x_train, x_test = np.split(mnist['data'],   [N])\n",
    "y_train, y_test = np.split(mnist['target'], [N])\n",
    "N_test = y_test.size\n",
    "\n",
    "# Prepare multi-layer perceptron model\n",
    "model = L.Classifier(MnistMLP(784, n_units, 10))\n",
    "if gpu >= 0:\n",
    "    cuda.get_device(gpu).use()\n",
    "    model.to_gpu()\n",
    "xp = np if gpu < 0 else cuda.cupy\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "graph generated\n",
      "train mean loss=72.77834922770658, accuracy=0.548199998922646, throughput=821.9231018705136 images/sec\n",
      "test  mean loss=219.76288055419923, accuracy=0.50469999730587\n",
      "epoch 2\n",
      "train mean loss=323.64375639279683, accuracy=0.5700666668762764, throughput=991.7653031961943 images/sec\n",
      "test  mean loss=361.8062232971191, accuracy=0.5763999950885773\n",
      "epoch 3\n",
      "train mean loss=886.6121348063151, accuracy=0.5786999992032846, throughput=1416.5538278092422 images/sec\n",
      "test  mean loss=1627.0319134521485, accuracy=0.44789999663829805\n",
      "epoch 4\n",
      "train mean loss=1411.1040188598633, accuracy=0.5954166663189729, throughput=1473.4830824416038 images/sec\n",
      "test  mean loss=888.3568920898438, accuracy=0.6919000005722046\n",
      "epoch 5\n",
      "train mean loss=1560.757921702067, accuracy=0.6334499986966451, throughput=1499.472335891493 images/sec\n",
      "test  mean loss=1180.1152792358398, accuracy=0.698100003004074\n",
      "epoch 6\n",
      "train mean loss=3162.0104974365236, accuracy=0.6203499992688497, throughput=1518.1738278961234 images/sec\n",
      "test  mean loss=1867.3983505249023, accuracy=0.7094999998807907\n",
      "epoch 7\n",
      "train mean loss=2650.8654927571615, accuracy=0.670099999755621, throughput=1521.9569651351426 images/sec\n",
      "test  mean loss=3903.0090075683593, accuracy=0.578999997973442\n",
      "epoch 8\n",
      "train mean loss=4505.263641866049, accuracy=0.6309833312531312, throughput=1529.7361208619857 images/sec\n",
      "test  mean loss=2808.2975787353516, accuracy=0.6823999989032745\n",
      "epoch 9\n",
      "train mean loss=4799.969349975586, accuracy=0.6552833340068658, throughput=1543.1117635974017 images/sec\n",
      "test  mean loss=7629.494633789062, accuracy=0.6685999986529351\n",
      "epoch 10\n",
      "train mean loss=6457.081641031901, accuracy=0.6428333327174187, throughput=1540.5867151828563 images/sec\n",
      "test  mean loss=4215.49250366211, accuracy=0.6569000005722045\n",
      "epoch 11\n",
      "train mean loss=7226.494430338542, accuracy=0.6537000000973543, throughput=1532.061450035015 images/sec\n",
      "test  mean loss=11890.813051757812, accuracy=0.6323999986052513\n",
      "epoch 12\n",
      "train mean loss=7911.202074788412, accuracy=0.6677666675547759, throughput=1545.4145496055655 images/sec\n",
      "test  mean loss=6487.900808105469, accuracy=0.654300001859665\n",
      "epoch 13\n",
      "train mean loss=9550.225048217773, accuracy=0.6583333329359691, throughput=1536.3131044364638 images/sec\n",
      "test  mean loss=21277.1288671875, accuracy=0.5850000002980232\n",
      "epoch 14\n",
      "train mean loss=10736.267187093099, accuracy=0.659483333081007, throughput=1535.1772034875937 images/sec\n",
      "test  mean loss=16305.636240234375, accuracy=0.5900999987125397\n",
      "epoch 15\n",
      "train mean loss=12866.425885416667, accuracy=0.6611166664958, throughput=1532.0258124595284 images/sec\n",
      "test  mean loss=17074.978930664063, accuracy=0.6475999993085861\n",
      "epoch 16\n",
      "train mean loss=10845.30354248047, accuracy=0.6928333348532518, throughput=1549.4108678910882 images/sec\n",
      "test  mean loss=19075.992329101562, accuracy=0.5515999987721443\n",
      "epoch 17\n",
      "train mean loss=15588.594213460286, accuracy=0.6670999988913536, throughput=92.87822768616502 images/sec\n",
      "test  mean loss=12257.130258789062, accuracy=0.6809000030159951\n",
      "epoch 18\n",
      "train mean loss=18222.298262532553, accuracy=0.6611333332955838, throughput=1069.8076431935388 images/sec\n",
      "test  mean loss=5544.125595703125, accuracy=0.8288999974727631\n",
      "epoch 19\n",
      "train mean loss=17391.96397644043, accuracy=0.6968166676163673, throughput=1295.4591204658518 images/sec\n",
      "test  mean loss=6783.839582519531, accuracy=0.8041999977827072\n",
      "epoch 20\n",
      "train mean loss=20829.603201090496, accuracy=0.6756499989330769, throughput=1143.5425279515312 images/sec\n",
      "test  mean loss=59551.606328125, accuracy=0.5541999989748001\n"
     ]
    }
   ],
   "source": [
    "# Learning loop\n",
    "for epoch in six.moves.range(1, n_epoch + 1):\n",
    "    print('epoch', epoch)\n",
    "\n",
    "    # training\n",
    "    perm = np.random.permutation(N)\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    start = time.time()\n",
    "    for i in six.moves.range(0, N, batchsize):\n",
    "        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]]))\n",
    "        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]]))\n",
    "\n",
    "        # Pass the loss function (Classifier defines it) and its arguments\n",
    "        optimizer.update(model, x, t)\n",
    "\n",
    "        if epoch == 1 and i == 0:\n",
    "            with open('graph.dot', 'w') as o:\n",
    "                variable_style = {'shape': 'octagon', 'fillcolor': '#E0E0E0',\n",
    "                                  'style': 'filled'}\n",
    "                function_style = {'shape': 'record', 'fillcolor': '#6495ED',\n",
    "                                  'style': 'filled'}\n",
    "                g = computational_graph.build_computational_graph(\n",
    "                    (model.loss, ),\n",
    "                    variable_style=variable_style,\n",
    "                    function_style=function_style)\n",
    "                o.write(g.dump())\n",
    "            print('graph generated')\n",
    "\n",
    "        sum_loss += float(model.loss.data) * len(t.data)\n",
    "        sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    throughput = N / elapsed_time\n",
    "    print('train mean loss={}, accuracy={}, throughput={} images/sec'.format(\n",
    "        sum_loss / N, sum_accuracy / N, throughput))\n",
    "\n",
    "    # evaluation\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    for i in six.moves.range(0, N_test, batchsize):\n",
    "        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]),\n",
    "                             volatile='on')\n",
    "        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]),\n",
    "                             volatile='on')\n",
    "        loss = model(x, t)\n",
    "        sum_loss += float(loss.data) * len(t.data)\n",
    "        sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
    "\n",
    "    print('test  mean loss={}, accuracy={}'.format(\n",
    "        sum_loss / N_test, sum_accuracy / N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the model\n",
      "save the optimizer\n"
     ]
    }
   ],
   "source": [
    "# Save the model and the optimizer\n",
    "print('save the model')\n",
    "serializers.save_npz('mlp.model', model)\n",
    "print('save the optimizer')\n",
    "serializers.save_npz('mlp.state', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
