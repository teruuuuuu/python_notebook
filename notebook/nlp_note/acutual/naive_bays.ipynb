{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "  \"\"\"Multinomial Naive Bayes\"\"\"\n",
    "  def __init__(self):\n",
    "    self.categories = set()     # カテゴリの集合\n",
    "    self.vocabularies = set()   # ボキャブラリの集合\n",
    "    self.wordcount = {}         # wordcount[cat][word] カテゴリでの単語の出現回数\n",
    "    self.catcount = {}          # catcount[cat] カテゴリの出現回数\n",
    "    self.denominator = {}       # denominator[cat] P(word|cat)の分母の値\n",
    "  \n",
    "  def train(self, data):\n",
    "    \"\"\"ナイーブベイズ分類器の訓練\"\"\"\n",
    "    # 文書集合からカテゴリを抽出して辞書を初期化\n",
    "    for d in data:\n",
    "      cat = d[0]\n",
    "      self.categories.add(cat)\n",
    "    for cat in self.categories:\n",
    "      self.wordcount[cat] = defaultdict(int)\n",
    "      self.catcount[cat] = 0\n",
    "    # 文書集合からカテゴリと単語をカウント\n",
    "    for d in data:\n",
    "      cat, doc = d[0], d[1:]\n",
    "      self.catcount[cat] += 1\n",
    "      for word in doc:\n",
    "        self.vocabularies.add(word)\n",
    "        self.wordcount[cat][word] += 1\n",
    "    # 単語の条件付き確率の分母の値をあらかじめ一括計算しておく（高速化のため）\n",
    "    for cat in self.categories:\n",
    "      self.denominator[cat] = sum(self.wordcount[cat].values()) + len(self.vocabularies)\n",
    " \n",
    "  def classify(self, doc):\n",
    "    \"\"\"事後確率の対数 log(P(cat|doc)) がもっとも大きなカテゴリを返す\"\"\"\n",
    "    best = None\n",
    "    max = -sys.maxsize\n",
    "    for cat in self.catcount.keys():\n",
    "      p = self.scoreCount(doc, cat)\n",
    "      if p > max:\n",
    "        max = p\n",
    "        best = cat\n",
    "    return best\n",
    "\n",
    "  def wordProb(self, word, cat):\n",
    "    \"\"\"単語の条件付き確率 P(word|cat) を求める\"\"\"\n",
    "    # ラプラススムージングを適用\n",
    "    # wordcount[cat]はdefaultdict(int)なのでカテゴリに存在しなかった単語はデフォルトの0を返す\n",
    "    # 分母はtrain()の最後で一括計算済み\n",
    "    return float(self.wordcount[cat][word] + 1) / float(self.denominator[cat])\n",
    "\n",
    "  def scoreCount(self, doc, cat):\n",
    "    \"\"\"文書が与えられたときのカテゴリの事後確率の対数 log(P(cat|doc)) を求める\"\"\"\n",
    "    total = sum(self.catcount.values())  # 総文書数\n",
    "    score = math.log(float(self.catcount[cat]) / total)  # log P(cat)\n",
    "    for word in doc:\n",
    "    # logをとるとかけ算は足し算になる\n",
    "      score += math.log(self.wordProb(word, cat))  # log P(word|cat)\n",
    "    return score\n",
    "\n",
    "  def saveTrain(self):\n",
    "    with open(\"categories.dump\", \"w\") as cat_f:\n",
    "      json.dump(str(self.categories), cat_f)\n",
    "    with open(\"vocabularies.dump\", \"w\") as voca_f:\n",
    "      json.dump(str(self.vocabularies), voca_f) \n",
    "    with open(\"wordcount.dump\", \"w\") as word_f:\n",
    "      json.dump(json.dumps(self.wordcount), word_f)\n",
    "    with open(\"catcount.dump\", \"w\") as cc_f:\n",
    "      json.dump(json.dumps(self.catcount), cc_f)\n",
    "    with open(\"denominator.dump\", \"w\") as denomi_f:\n",
    "      json.dump(json.dumps(self.denominator), denomi_f)\n",
    "    return\n",
    "\n",
    "  def loadTrain(self):\n",
    "    with open(\"categories.dump\", \"r\") as cat_f:\n",
    "      self.categories = map(lambda n:n.strip()[1:-1],\n",
    "          json.load(cat_f)[1:-1].split(\",\"))\n",
    "    with open(\"vocabularies.dump\", \"r\") as voca_f:\n",
    "      self.vocabularies = map(lambda n:n.strip()[1:-1],\n",
    "          json.load(voca_f)[1:-1].split(\",\"))\n",
    "    with open(\"wordcount.dump\", \"r\") as word_f:\n",
    "      self.wordcount = json.loads(json.load(word_f))\n",
    "    with open(\"catcount.dump\", \"r\") as cc_f:\n",
    "      self.catcount = json.loads(json.load(cc_f))\n",
    "    with open(\"denominator.dump\", \"r\") as denomi_f:\n",
    "      self.denominator = json.loads(json.load(denomi_f))\n",
    "    return\n",
    "\n",
    "  def __str__(self):\n",
    "    total = sum(self.catcount.values())  # 総文書数\n",
    "    return \"documents: %d, vocabularies: %d, categories: %d\" % (total, len(self.vocabularies), len(self.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 4, vocabularies: 6, categories: 2\n",
      "P(Chinese|yes) =  0.42857142857142855\n",
      "P(Tokyo|yes) =  0.07142857142857142\n",
      "P(Japan|yes) =  0.07142857142857142\n",
      "P(Chinese|no) =  0.2222222222222222\n",
      "P(Tokyo|no) =  0.2222222222222222\n",
      "P(Japan|no) =  0.2222222222222222\n",
      "log P(yes|test) = -8.10769031284391\n",
      "log P(no|test) = -8.906681345001262\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# Introduction to Information Retrieval 13.2の例題\n",
    "data = [[\"yes\", \"Chinese\", \"Beijing\", \"Chinese\"],\n",
    "[\"yes\", \"Chinese\", \"Chinese\", \"Shanghai\"],\n",
    "[\"yes\", \"Chinese\", \"Macao\"],\n",
    "[\"no\", \"Tokyo\", \"Japan\", \"Chinese\"]]\n",
    "# ナイーブベイズ分類器を訓練\n",
    "nb = NaiveBayes()\n",
    "nb.train(data)\n",
    "print (nb)\n",
    "print (\"P(Chinese|yes) = \", nb.wordProb(\"Chinese\", \"yes\"))\n",
    "print (\"P(Tokyo|yes) = \", nb.wordProb(\"Tokyo\", \"yes\"))\n",
    "print (\"P(Japan|yes) = \", nb.wordProb(\"Japan\", \"yes\"))\n",
    "print (\"P(Chinese|no) = \", nb.wordProb(\"Chinese\", \"no\"))\n",
    "print (\"P(Tokyo|no) = \", nb.wordProb(\"Tokyo\", \"no\"))\n",
    "print (\"P(Japan|no) = \", nb.wordProb(\"Japan\", \"no\"))\n",
    "# テストデータのカテゴリを予測\n",
    "test = [\"Chinese\", \"Chinese\", \"Chinese\", \"Tokyo\", \"Japan\"]\n",
    "print (\"log P(yes|test) =\", nb.scoreCount(test, \"yes\"))\n",
    "print (\"log P(no|test) =\", nb.scoreCount(test, \"no\"))\n",
    "print (nb.classify(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.saveTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.loadTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(yes|test) = -8.10769031284391\n",
      "log P(no|test) = -8.906681345001262\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "test = [\"Chinese\", \"Chinese\", \"Chinese\", \"Tokyo\", \"Japan\"]\n",
    "print (\"log P(yes|test) =\", nb.scoreCount(test, \"yes\"))\n",
    "print (\"log P(no|test) =\", nb.scoreCount(test, \"no\"))\n",
    "print (nb.classify(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
